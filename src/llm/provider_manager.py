"""
LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
è¤‡æ•°ã®ç„¡æ–™LLMã‚µãƒ¼ãƒ“ã‚¹ã‚’åŠ¹ç‡çš„ã«ç®¡ç†ã—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã‚’æä¾›
"""

import asyncio
import os
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from collections import defaultdict

# ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
try:
    import openai
except ImportError:
    openai = None

try:
    import google.generativeai as genai
except ImportError:
    genai = None

try:
    from groq import Groq
except ImportError:
    Groq = None

from .rate_limiter import RateLimiter
from .fallback_handler import FallbackHandler
from ..utils.cache_manager import ResponseCache

class LLMProvider:
    """åŸºåº•LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.config = config
        self.client = None
        self.is_available = False
        
    async def initialize(self) -> bool:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–"""
        raise NotImplementedError
        
    async def get_completion(self, prompt: str, **kwargs) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®å®Ÿè¡Œ"""
        raise NotImplementedError
        
    def is_healthy(self) -> bool:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        return self.is_available

class GoogleGeminiProvider(LLMProvider):
    """Google Gemini ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    
    async def initialize(self) -> bool:
        try:
            if not genai:
                logging.error("google-generativeai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
                
            api_key = os.getenv(self.config.get('api_key_env', 'GOOGLE_API_KEY'))
            if not api_key:
                logging.error("Google API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
                
            genai.configure(api_key=api_key)
            self.client = genai.GenerativeModel(self.config.get('model', 'gemini-1.5-flash'))
            self.is_available = True
            
            logging.info(f"âœ… {self.name} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
            return True
            
        except Exception as e:
            logging.error(f"âŒ {self.name} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            return False
    
    async def get_completion(self, prompt: str, **kwargs) -> str:
        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None, 
                lambda: self.client.generate_content(prompt)
            )
            return response.text
            
        except Exception as e:
            logging.error(f"Google Gemini ã‚¨ãƒ©ãƒ¼: {e}")
            raise

class GroqProvider(LLMProvider):
    """Groq ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    
    async def initialize(self) -> bool:
        try:
            if not Groq:
                logging.error("groq ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
                
            api_key = os.getenv(self.config.get('api_key_env', 'GROQ_API_KEY'))
            if not api_key:
                logging.error("Groq API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
                
            self.client = Groq(api_key=api_key)
            self.is_available = True
            
            logging.info(f"âœ… {self.name} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
            return True
            
        except Exception as e:
            logging.error(f"âŒ {self.name} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            return False
    
    async def get_completion(self, prompt: str, **kwargs) -> str:
        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model=self.config.get('model', 'llama3-70b-8192'),
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=self.config.get('max_tokens', 4096),
                    temperature=self.config.get('temperature', 0.3)
                )
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logging.error(f"Groq ã‚¨ãƒ©ãƒ¼: {e}")
            raise

class TogetherAIProvider(LLMProvider):
    """Together AI ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    
    async def initialize(self) -> bool:
        try:
            if not openai:
                logging.error("openai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
                
            api_key = os.getenv(self.config.get('api_key_env', 'TOGETHER_API_KEY'))
            if not api_key:
                logging.error("Together AI API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return False
                
            self.client = openai.OpenAI(
                api_key=api_key,
                base_url=self.config.get('base_url', 'https://api.together.xyz/v1')
            )
            self.is_available = True
            
            logging.info(f"âœ… {self.name} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
            return True
            
        except Exception as e:
            logging.error(f"âŒ {self.name} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
            return False
    
    async def get_completion(self, prompt: str, **kwargs) -> str:
        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.client.chat.completions.create(
                    model=self.config.get('model', 'meta-llama/Llama-3-8b-chat-hf'),
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=self.config.get('max_tokens', 2048),
                    temperature=self.config.get('temperature', 0.5)
                )
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logging.error(f"Together AI ã‚¨ãƒ©ãƒ¼: {e}")
            raise

class LLMProviderManager:
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.providers: Dict[str, LLMProvider] = {}
        self.rate_limiter = RateLimiter()
        self.fallback_handler = FallbackHandler()
        self.cache = ResponseCache()
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å„ªå…ˆé †ä½
        self.provider_priority = [
            "google_gemini",
            "groq_llama", 
            "together_ai"
        ]
        
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥æœ€é©ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        self.task_provider_map = {
            "code_generation": "groq_llama",
            "complex_reasoning": "google_gemini", 
            "simple_task": "together_ai",
            "general": "google_gemini"
        }
        
    async def initialize(self) -> bool:
        """å…¨ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆæœŸåŒ–"""
        logging.info("ğŸš€ LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­...")
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è¨­å®šã‹ã‚‰å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
        provider_configs = self.config.get('providers', {})
        
        for provider_name, provider_config in provider_configs.items():
            try:
                if provider_name == 'google_gemini':
                    provider = GoogleGeminiProvider(provider_name, provider_config)
                elif provider_name == 'groq_llama':
                    provider = GroqProvider(provider_name, provider_config)
                elif provider_name == 'together_ai':
                    provider = TogetherAIProvider(provider_name, provider_config)
                else:
                    logging.warning(f"æœªçŸ¥ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider_name}")
                    continue
                    
                if await provider.initialize():
                    self.providers[provider_name] = provider
                    logging.info(f"âœ… {provider_name} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–å®Œäº†")
                else:
                    logging.warning(f"âš ï¸ {provider_name} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–å¤±æ•—")
                    
            except Exception as e:
                logging.error(f"âŒ {provider_name} ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        
        if not self.providers:
            logging.error("âŒ åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“")
            return False
            
        logging.info(f"âœ… {len(self.providers)}å€‹ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
        return True
    
    def _select_provider(self, task_type: str = "general") -> Optional[str]:
        """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãæœ€é©ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ"""
        
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãæœ€é©ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        preferred_provider = self.task_provider_map.get(task_type, "google_gemini")
        
        # æœ€é©ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        if (preferred_provider in self.providers and 
            self.providers[preferred_provider].is_healthy() and
            self.rate_limiter.can_make_request(preferred_provider)):
            return preferred_provider
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å„ªå…ˆé †ä½é †ã«ãƒã‚§ãƒƒã‚¯
        for provider_name in self.provider_priority:
            if (provider_name in self.providers and 
                self.providers[provider_name].is_healthy() and
                self.rate_limiter.can_make_request(provider_name)):
                return provider_name
        
        return None
    
    async def get_completion(self, prompt: str, task_type: str = "general", **kwargs) -> str:
        """æœ€é©ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§LLMæ¨è«–å®Ÿè¡Œ"""
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cached_response = self.cache.get_cached_response(prompt)
        if cached_response:
            logging.info("ğŸ’° ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—")
            return cached_response
        
        # æœ€é©ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
        selected_provider = self._select_provider(task_type)
        
        if not selected_provider:
            raise Exception("åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“")
        
        try:
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
            provider = self.providers[selected_provider]
            response = await provider.get_completion(prompt, **kwargs)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ›´æ–°
            self.rate_limiter.record_request(selected_provider)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            self.cache.cache_response(prompt, response)
            
            logging.info(f"âœ… {selected_provider} ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆå®Œäº†")
            return response
            
        except Exception as e:
            logging.error(f"âŒ {selected_provider} ã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
            return await self._fallback_request(prompt, task_type, excluded=[selected_provider])
    
    async def _fallback_request(self, prompt: str, task_type: str, excluded: List[str] = None) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ"""
        if excluded is None:
            excluded = []
            
        logging.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œä¸­...")
        
        for provider_name in self.provider_priority:
            if (provider_name not in excluded and
                provider_name in self.providers and
                self.providers[provider_name].is_healthy()):
                
                try:
                    provider = self.providers[provider_name]
                    response = await provider.get_completion(prompt)
                    
                    self.rate_limiter.record_request(provider_name)
                    self.cache.cache_response(prompt, response)
                    
                    logging.info(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ: {provider_name}")
                    return response
                    
                except Exception as e:
                    logging.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•— {provider_name}: {e}")
                    continue
        
        raise Exception("å…¨ã¦ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
    
    def get_provider_status(self) -> Dict[str, Dict[str, Any]]:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®çŠ¶æ…‹å–å¾—"""
        status = {}
        
        for name, provider in self.providers.items():
            status[name] = {
                'available': provider.is_healthy(),
                'requests_today': self.rate_limiter.get_daily_requests(name),
                'rate_limit_status': self.rate_limiter.get_status(name)
            }
        
        return status